{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CVAL on-premice YOLOv5 advanced example"
      ],
      "metadata": {
        "id": "w3kj45ffWEDf"
      },
      "id": "w3kj45ffWEDf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "rPLpq93AF9-r"
      },
      "id": "rPLpq93AF9-r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "133d3d0c",
      "metadata": {
        "id": "133d3d0c"
      },
      "outputs": [],
      "source": [
        "%cd .\n",
        "!ls\n",
        "# Optional for local:\n",
        "# !apt-get install git -y\n",
        "!git clone https://github.com/fangorntreabeard/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -q pydantic==1.10.9\n",
        "!pip install -qr requirements.txt\n",
        "!pip install -q cval-lib\n",
        "!pip install -q git+https://github.com/fangorntreabeard/CLIP.git\n",
        "%cd ..\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up your user_api_key\n",
        "\n",
        "USER_API_KEY = ..."
      ],
      "metadata": {
        "id": "n11FMbvHSoZ7"
      },
      "id": "n11FMbvHSoZ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structure of data\n",
        "    data\n",
        "        dataset_name\n",
        "            all_train_images\n",
        "            all_train_labels\n",
        "            train\n",
        "                labels\n",
        "                    annotations in YOLOV5 format\n",
        "                images\n",
        "            test\n",
        "                labels\n",
        "                    annotations in YOLOV5 format\n",
        "                images\n",
        "            val\n",
        "                labels\n",
        "                   annotations in YOLOV5 format\n",
        "                images\n",
        "        data/data.yml\n",
        "    yolov5/yolov5s.pt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TEk9eiovG-bS"
      },
      "id": "TEk9eiovG-bS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main class for experiments\n"
      ],
      "metadata": {
        "id": "SidD_7-cK2VF"
      },
      "id": "SidD_7-cK2VF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ad8f9f",
      "metadata": {
        "id": "74ad8f9f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from contextlib import suppress\n",
        "import random\n",
        "import json\n",
        "import shutil\n",
        "from typing import List\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "from yolov5 import train, val, detect\n",
        "from cval_lib.connection import CVALConnection\n",
        "\n",
        "\n",
        "class YOLOWorker:\n",
        "    # config\n",
        "    def __init__(self, path_to_dataset: str, model: str = None):\n",
        "        self.path_to_data = Path(path_to_dataset).parent\n",
        "        self.dataset = Path(path_to_dataset).name\n",
        "        self.dataset_conf = self.path_to_data / 'data.yaml'\n",
        "        self.model = model\n",
        "        self.weights = None\n",
        "        self.all_train_images = self.path_to_data / self.dataset / 'all_train_images'\n",
        "\n",
        "    # copy dataset/train/labels data\n",
        "    def annotate_dataset(self, img_ids: List[str], random_sample = None):\n",
        "        ids = set(\n",
        "            map(\n",
        "                lambda x: Path(x).name.replace('.txt', ''),\n",
        "                os.listdir(\n",
        "                    Path(self.path_to_data) / self.dataset / 'all_train_labels'\n",
        "                )\n",
        "            )\n",
        "        ).intersection(set(img_ids))\n",
        "        if random_sample is not None:\n",
        "          ids = random.sample(list(ids), random_sample)\n",
        "        labels = Path(self.path_to_data) / self.dataset / 'train/labels'\n",
        "        images = Path(self.path_to_data) / self.dataset / 'train/images'\n",
        "        # remove cache\n",
        "        cache = Path(self.path_to_data) / self.dataset / 'train/labels.cache'\n",
        "        counter = 0\n",
        "        with suppress(Exception):\n",
        "          shutil.rmtree(labels)\n",
        "        with suppress(Exception):\n",
        "          shutil.rmtree(images)\n",
        "        with suppress(Exception):\n",
        "          cache.unlink()\n",
        "        os.makedirs(labels, exist_ok=True)\n",
        "        os.makedirs(images, exist_ok=True)\n",
        "        for i in ids:\n",
        "            shutil.copyfile(\n",
        "                Path(Path(self.path_to_data) / self.dataset / 'all_train_labels') / (i+'.txt'),\n",
        "                Path(Path(self.path_to_data) / self.dataset / 'train/labels') / (i+'.txt'),\n",
        "            )\n",
        "            shutil.copyfile(\n",
        "                Path(self.all_train_images) / (i+'.jpg'),\n",
        "                Path(Path(self.path_to_data) / self.dataset / 'train/images') / (i+'.jpg'),\n",
        "            )\n",
        "\n",
        "    # start training\n",
        "    def train(\n",
        "            self,\n",
        "            device: int = 0,\n",
        "            batch: int = -1,\n",
        "            log_tag: str = 'default',\n",
        "            epochs: int = 100,\n",
        "            pretrain: bool = False,Ñ‡\n",
        "            workers=50,\n",
        "            model: str = None,\n",
        "    ):\n",
        "        self.model = f'{uuid.uuid4()}' if model is None else model\n",
        "        self.weights = self.path_to_data / self.dataset / f'calls/{self.model}/exp/weights/best.pt'\n",
        "        train.run(\n",
        "            workers=workers,\n",
        "            weights='yolov5/yolov5s.pt',\n",
        "            data=self.dataset_conf,\n",
        "            epochs=epochs,\n",
        "            pretrained=pretrain,\n",
        "            nosave=False,\n",
        "            batch=batch,\n",
        "            device=device,\n",
        "            project=self.path_to_data/ self.dataset / \"calls\" / self.model,\n",
        "        )\n",
        "        self.write_log(log_tag)\n",
        "\n",
        "    # get test values\n",
        "    def test(\n",
        "        self,\n",
        "        log_tag: str = 'default',\n",
        "        device: int = 0,\n",
        "        workers=50,\n",
        "        batch_size=50,\n",
        "    ):\n",
        "        time.sleep(10)\n",
        "        with suppress(FileNotFoundError):\n",
        "            shutil.rmtree(self.path_to_data / self.dataset / f'calls/{self.model}/exp' / 'val')\n",
        "        res, _, _ = val.run(\n",
        "            weights=self.weights,\n",
        "            data=self.dataset_conf,\n",
        "            task='test',\n",
        "            workers=workers,\n",
        "            save_txt=True,\n",
        "            save_conf=True,\n",
        "            device=device,\n",
        "            project=self.path_to_data/ self.dataset / \"calls\" / self.model,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        self.write_log(log_tag)\n",
        "        return res\n",
        "\n",
        "    def write_log(self, log_tag):\n",
        "        if not Path(f\"{self.path_to_data}/logs\").exists():\n",
        "            os.makedirs(Path(f\"{self.path_to_data}/logs\"), exist_ok=True)\n",
        "        with open(f\"{self.path_to_data}/logs/{log_tag}.log\", 'a+') as f:\n",
        "            f.write(self.model + '\\n')\n",
        "\n",
        "    # get bbox scores\n",
        "    def detect(\n",
        "            self,\n",
        "            device: int = 0,\n",
        "            log_tag: str = 'default',\n",
        "            rm=True,\n",
        "    ):\n",
        "        time.sleep(2)\n",
        "        with suppress(Exception):\n",
        "            os.remove('yolov5/confidence.txt')\n",
        "        detect.run(\n",
        "            data=self.dataset_conf,\n",
        "            weights=self.weights,\n",
        "            source=self.all_train_images,\n",
        "            project=self.path_to_data/ self.dataset / \"calls\" / self.model,\n",
        "            exist_ok=True,\n",
        "            device=device,\n",
        "            nosave=True,\n",
        "            save_conf=True,\n",
        "            save_txt=True,\n",
        "        )\n",
        "        self.write_log(log_tag)\n",
        "        return self.bbox_score()\n",
        "\n",
        "    # get IDs of images\n",
        "    @property\n",
        "    def ids(self):\n",
        "        return list(map(lambda x: Path(x).name.replace('.jpg', ''), os.listdir(self.all_train_images)))\n",
        "\n",
        "    @staticmethod\n",
        "    def _score_parser(f):\n",
        "        return list(map(lambda x: {'frame_id': x[0],  'category_id': int(x[1]), 'score': float(x[-1])}, [i.split('\\t') for i in frozenset(f.readlines())]))\n",
        "\n",
        "    def bbox_score(self):\n",
        "        try:\n",
        "            with open(f'yolov5/confidence.txt') as f:\n",
        "                return self._score_parser(f)\n",
        "        except:\n",
        "            print(\"No detections!\")\n",
        "            return []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib as plb\n",
        "import uuid\n",
        "from typing import Union\n",
        "\n",
        "import clip\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from cval_lib.models.embedding import FrameEmbeddingModel, EmbeddingModel\n",
        "\n",
        "# embedding and score generator for schemes FrameEmbeddingModel, EmbeddingModel\n",
        "\n",
        "def get_frames(imgs_path: Union[os.PathLike, str, plb.Path], weights_path: Union[os.PathLike, str, plb.Path]):\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_yolo = torch.hub.load('ultralytics/yolov5', 'custom', path=weights_path)\n",
        "    model_clip, preprocess = clip.load(\"RN101\", device=device)\n",
        "    names_imgs = os.listdir(imgs_path)\n",
        "    list_imgs = [os.path.join(imgs_path, path) for path in names_imgs]\n",
        "    tabs = []\n",
        "    imgs_numpy = []\n",
        "\n",
        "    for batch in range(len(list_imgs) // 10 + 1):\n",
        "        bb = model_yolo(list_imgs[batch * 10: (batch + 1) * 10])\n",
        "        tabs = tabs + [x.detach().cpu().numpy() for x in bb.xyxy]\n",
        "        imgs_numpy = imgs_numpy + bb.ims\n",
        "    n_images = len(tabs)\n",
        "    for i in range(n_images):\n",
        "        if len(tabs[i]) == 0:\n",
        "            tabs[i] = np.array(\n",
        "                [\n",
        "                    [0, 0, imgs_numpy[i].shape[0], imgs_numpy[i].shape[1], 1, -1],\n",
        "                ],\n",
        "            )\n",
        "    n_images = len(tabs)\n",
        "    embeddings = []\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(n_images):\n",
        "        emb = []\n",
        "        scr = []\n",
        "        for boxs in range(len(tabs[i])):\n",
        "            x1 = int(tabs[i][boxs, 0])\n",
        "            y1 = int(tabs[i][boxs, 1])\n",
        "            x2 = int(tabs[i][boxs, 2])\n",
        "            y2 = int(tabs[i][boxs, 3])\n",
        "            im = Image.fromarray(imgs_numpy[i][y1: y2, x1: x2])\n",
        "            im.save('test.jpg')\n",
        "            image = preprocess(Image.open(\"test.jpg\")).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _id = uuid.uuid4().hex\n",
        "                image_features = model_clip.encode_image(image)\n",
        "                image_features = [float(x) for x in image_features.detach().cpu().numpy()[0].tolist()]\n",
        "                scr.append(\n",
        "                    {\n",
        "                        \"embedding_id\": _id,\n",
        "                        \"score\": float(tabs[i][boxs, 4]) + 0.01,\n",
        "                        \"category_id\": int(tabs[i][boxs, 5]),\n",
        "                    },\n",
        "                )\n",
        "                emb.append(\n",
        "                    EmbeddingModel(**{\n",
        "                        \"embedding_id\": _id,\n",
        "                        \"embedding\": image_features,\n",
        "                    })\n",
        "                )\n",
        "\n",
        "        embeddings.append(\n",
        "            FrameEmbeddingModel(**{\n",
        "                \"frame_id\": names_imgs[i].split('.')[0],\n",
        "                \"embeddings\": emb,\n",
        "            }),\n",
        "        )\n",
        "        predictions.append(\n",
        "            {\n",
        "                \"frame_id\": names_imgs[i].split('.')[0],\n",
        "                \"predictions\": scr,\n",
        "            },\n",
        "        )\n",
        "    return embeddings, predictions"
      ],
      "metadata": {
        "id": "ucWDV-FXOKQF"
      },
      "id": "ucWDV-FXOKQF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "aRXMfrVjKhBX"
      },
      "id": "aRXMfrVjKhBX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "ImBEYqY9Kns8"
      },
      "id": "ImBEYqY9Kns8"
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATASET = ...\n",
        "LINES_COUNT = ...\n",
        "DETECTOR = CVALConnection(USER_API_KEY).detection()\n",
        "STEPS = ...\n",
        "\n",
        "for _ in range(LINES_COUNT):\n",
        "    yolo = YOLOWorker(PATH_TO_DATASET)\n",
        "    train_size = len(yolo.ids)\n",
        "    step = train_size // STEPS\n",
        "    ids = []\n",
        "    for sample_size in range(step, step*(STEPS+1), step):\n",
        "        # create ranodm sample\n",
        "        ids += random.sample(list(filter(lambda x: x not in ids, yolo.ids)), step)\n",
        "        yolo.annotate_dataset(ids)\n",
        "        yolo.train(\n",
        "            epochs=100,\n",
        "            workers=150,\n",
        "        )\n",
        "        with open('result_random.csv', 'a+') as f:\n",
        "            f.write(f'{time.time()},{\",\".join(list(map(str, yolo.test(workers=1))},{sample_size};\\n')"
      ],
      "metadata": {
        "id": "VzVCE-OjKR8T"
      },
      "id": "VzVCE-OjKR8T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy sampling"
      ],
      "metadata": {
        "id": "l6lnOBqhSSVK"
      },
      "id": "l6lnOBqhSSVK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a53e377",
      "metadata": {
        "id": "6a53e377"
      },
      "outputs": [],
      "source": [
        "from cval_lib.models.detection import BBoxScores, FramePrediction\n",
        "from cval_lib.models.detection import DetectionSamplingOnPremise\n",
        "\n",
        "PATH_TO_DATASET = ...\n",
        "LINES_COUNT = ...\n",
        "DETECTOR = CVALConnection(USER_API_KEY).detection()\n",
        "STEPS = ...\n",
        "\n",
        "for _ in range(LINES_COUNT):\n",
        "    yolo = YOLOWorker(PATH_TO_DATASET)\n",
        "    ids = yolo.ids\n",
        "    train_size = len(ids)\n",
        "    step = train_size // STEPS\n",
        "    ids = random.sample(ids, step)\n",
        "    yolo.annotate_dataset(ids)\n",
        "\n",
        "    for sample_size in range(step, step*(STEPS+1), step):\n",
        "        delta = []\n",
        "        yolo.train(epochs=100, workers=1, pretrain=True)\n",
        "        od_res = [i for i in yolo.detect() if i not in ids]\n",
        "        # if YOLOV5 detections are not empty, create request with predictions\n",
        "        if od_res:\n",
        "            frames_pred = list(map(lambda x: {'frame_id': x, 'predictions': []}, set(filter(lambda x: x not in ids, map(lambda x: x.get('frame_id'), od_res)))))\n",
        "            for (i, frame) in enumerate(frames_pred):\n",
        "                frames_pred[i]['predictions'] = frames_pred[i]['predictions'] + list(\n",
        "                    map(lambda x: x, filter(lambda x: x.get('frame_id') == frame.get('frame_id'), od_res))\n",
        "                )\n",
        "            frames_pred = list(filter(lambda x: x.get('predictions'), frames_pred))\n",
        "            request = DetectionSamplingOnPremise(\n",
        "                 num_of_samples=step if step <= len(frames_pred) else len(frames_pred),\n",
        "                 bbox_selection_policy='sum',\n",
        "                 selection_strategy='entropy',\n",
        "                 sort_strategy='ascending',\n",
        "                 frames=frames_pred,\n",
        "                )\n",
        "            delta = list(map(lambda x: x, DETECTOR.on_premise_sampling(request).result))\n",
        "        # add random\n",
        "        delta += random.sample(list(set(yolo.ids) - set(delta) - set(ids)), step-len(delta))\n",
        "        # add delta\n",
        "        # write result\n",
        "        with open('result_entropy.csv', 'a+') as f:\n",
        "            f.write(f'{time.time()},{\",\".join(list(map(str, yolo.test(workers=1))))},{sample_size};\\n')\n",
        "        ids += delta\n",
        "        yolo.annotate_dataset(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering sampling"
      ],
      "metadata": {
        "id": "2ebJCgi6SHcl"
      },
      "id": "2ebJCgi6SHcl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4b81d9",
      "metadata": {
        "id": "0e4b81d9"
      },
      "outputs": [],
      "source": [
        "from cval_lib.connection import CVALConnection\n",
        "\n",
        "from cval_lib.models.detection import BBoxScores, FramePrediction\n",
        "from cval_lib.models.detection import DetectionSamplingOnPremise\n",
        "from cval_lib.connection import CVALConnection\n",
        "\n",
        "PATH_TO_DATASET = ...\n",
        "YOLO = YOLOWorker(PATH_TO_DATASET)\n",
        "PATH_TO_ALL_TRAIN_IMAGES = YOLO.all_train_images\n",
        "LINES_COUNT = ...\n",
        "DETECTOR = CVALConnection(USER_API_KEY)\n",
        "STEPS = ...\n",
        "\n",
        "for _ in range(LINES_COUNT):\n",
        "    ids = YOLO.ids\n",
        "    step = 250\n",
        "    ids = random.sample(ids, step)\n",
        "    YOLO.annotate_dataset(ids)\n",
        "    YOLO.train(epochs=epoch, pretrain=True)\n",
        "    ds_id = detector.dataset().create(name='b', description='b')\n",
        "    for sample_size in range(step, step * STEPS, step):\n",
        "        embeddings, predictions = get_frames(YOLO.all_train_images, YOLO, ids)\n",
        "        if len(predictions) > step:\n",
        "            detector.embedding(dataset_id=ds_id, part_of_dataset='training').upload_many(embeddings)\n",
        "            result = detector.detection().on_premise_sampling(\n",
        "                DetectionSamplingOnPremise(\n",
        "                 num_of_samples=step,\n",
        "                 selection_strategy='clustering',\n",
        "                 dataset_id=ds_id,\n",
        "                 frames=predictions,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            while result.result is None:\n",
        "                result = detector.result().get(result.task_id)\n",
        "            ids += list(\n",
        "                filter(lambda i: i not in ids, result.result),\n",
        "            )\n",
        "        else:\n",
        "            good_id = [x[\"frame_id\"] for x in predictions]\n",
        "            delta = good_id + random.sample(list(set(YOLO.ids) - set(good_id) - set(ids)), k=step-len(good_id))\n",
        "\n",
        "            ids += delta\n",
        "        with open('result_cluster.csv', 'a+') as f:\n",
        "            f.write(f'{time.time()},{\",\".join(list(map(str, YOLO.test())))},{sample_size};\\n')\n",
        "        YOLO.annotate_dataset(ids)\n",
        "        if sample_size != step * (STEPS-1):\n",
        "            YOLO.train(epochs=epoch, pretrain=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "uvLE7qitT_NX"
      },
      "id": "uvLE7qitT_NX"
    },
    {
      "cell_type": "code",
      "source": [
        "# get results\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "for pth in (\n",
        "    'result_clustering.csv',\n",
        "    'result_entropy.csv',\n",
        "    'result_random.csv',\n",
        "):\n",
        "  if Path(pth).exists():\n",
        "    with open(pth) as res:\n",
        "      print(\n",
        "            pth,\n",
        "            res.read(),\n",
        "            sep=f\"\\n{'_'*len(pth)}\\n\\n\",\n",
        "      )"
      ],
      "metadata": {
        "id": "89c2DFZTTSWQ"
      },
      "id": "89c2DFZTTSWQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}